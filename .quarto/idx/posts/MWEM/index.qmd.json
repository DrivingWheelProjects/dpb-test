{"title":"MWEM","markdown":{"yaml":{"title":"MWEM","jupyter":"python3","format":{"html":{"code-fold":true}}},"headingText":"The Private Data","containsRefs":false,"markdown":"\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport random\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef laplace_mech(v, sensitivity, epsilon):\n    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n\ndef laplace_mech_vec(vec, sensitivity, epsilon):\n    return [v + np.random.laplace(loc=0, scale=sensitivity / epsilon) for v in vec]\n\ndef gaussian_mech(v, sensitivity, epsilon, delta):\n    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n\ndef gaussian_mech_vec(vec, sensitivity, epsilon, delta):\n    return [v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n            for v in vec]\n\ndef pct_error(orig, priv):\n    return np.abs(orig - priv)/orig * 100.0\n\nadult = pd.read_csv('https://github.com/jnear/cs3110-data-privacy/raw/main/homework/adult_with_pii.csv')\n\ndef report_noisy_max(data, options, score_function, sensitivity, epsilon):\n    # Calculate the score for each element of R\n    scores = [score_function(data, r) for r in options]\n\n    # Add noise to each score\n    noisy_scores = [laplace_mech(score, sensitivity, epsilon) for score in scores]\n\n    # Find the index of the maximum score\n    max_idx = np.argmax(noisy_scores)\n\n    # Return the element corresponding to that index\n    return options[max_idx]\n```\n\n\nThe Multiplicative Weights with Exponential Mechanism (MWEM) mechanism is a differentially private, workload-aware algorithm for modeling the distribution of a private dataset. It can be used to answer query workloads, to generate synthetic data, or to build models that could be used for other purposes. The algorithm was original presented in the paper [A Simple and Practical Algorithm for Differentially Private Data Release](https://proceedings.neurips.cc/paper_files/paper/2012/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf) by Moritz Hardt, Katrina Ligett, and Frank McSherry. MWEM has remained an influential approach since its introduction, since it's simple and easy to implement but also works well in many cases.\n\nThe complete algorithm (screenshotted from the paper) appears below, in Figure 1. The inputs to the algorithm are:\n\n- The (private) dataset $B$\n- A set $Q$ of linear queries, often called the query workload\n- The number of iterations $T$ to run the algorithm\n- The privacy budget $\\epsilon$\n- The number of data records $n$\n\nThe algorithm outputs a *probability distribution* over all possible data points in $D$. Sampling from this distribution yields synthetic data; it can also be used directly to answer arbitrary linear queries. The rest of this post describes and implements the pieces of the algorithm.\n\n\n![ss.png](attachment:ss.png)\n\n\nThe algorithm's first argument, $B$, is a private dataset. For our implementation, we'll use the `Age` column of the Adult dataset.\n\n```{python}\nB = adult['Age']\nB.hist();\n```\n\n# The Query Workload\n\nThe algorithm's second argument, $Q$, is a set of linear queries. Here, a *linear* query refers to a query that can be written as a weighted sum of elements; counts and sums are typical examples of linear queries. $Q$ in the algorithm is a set of these queries, and is often called a *query workload* in other papers. \n\nIn a workload-aware algorithm like MWEM, the goal is to build a model that does a good job answering the queries in the workload, under the assumption that the analyst has used domain-specific knowledge to carefully design the workload so it includes all of the most important queries. In practice, generating a good workload is actually really difficult, because most analysts don't know exactly what the data will be used for when building the differentially private data release. Most papers use simple stand-ins for the workload, like 2- or 3-way marginal queries, prefix queries, or range queries.\n\nOur implementation supports arbitrary linear queries, but we'll use random [range queries](https://programming-dp.com/ch10.html#application-range-queries) as our workload. Specifically, our range queries will count the number of individuals in the dataset whose ages lie within an interval. The `range_query` function implements a range query on a pandas dataframe.\n\n```{python}\n#| colab: {base_uri: https://localhost:8080/}\ndef range_query(data, q):\n    lower, upper = q\n    return len(data[(data >= lower) & (data < upper)])\n```\n\nWe generate 100 random range queries for the `Age` column of the Adult dataset. We represent the queries using the upper and lower bound for each range. We can run these queries by calling the `range_query` function: the code below generates the queries, runs `range_query` to get the answers, and prints the first 5 elements of each.\n\n```{python}\nNUM_QUERIES = 100\nrandom_lower_bounds = [random.randint(1, 70) for _ in range(NUM_QUERIES)]\nQ = [(lb, random.randint(lb, 100)) for lb in random_lower_bounds]\nreal_answers = [range_query(B, q) for q in Q]\nprint('First 5 queries: ', Q[:5])\nprint('First 5 answers: ', real_answers[:5])\n```\n\n# Step 0: Initialize the Model\n\nThe first step of the algorithm says: \"Let $A_0$ denote $n$ times the uniform distribution over $D$.\" What does that mean?\n\nIn our case, where the dataset is the `Age` column of the Adult dataset, $D$ is the universe of all possible ages. If we assume nobody is older than 100 years old, then we could use the numbers from 0 to 100 for our universe.\n\n```{python}\nD = list(range(0, 100))\n```\n\nSince $D$ is finite, we can represent a probability distribution over $D$ as a mapping from each element of $D$ to its probability (i.e. a [probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function) (PMF)). The PMF for the uniform distribution in this case assigns equal probability to each element of $D$. Since we have 100 ages, the probability for each one is 1%. Finally, $n$ times this distribution scales each entry by $n$.\n\n```{python}\nn = len(adult)\nuniform_D = {x: 1/len(D) for x in D} # give each record equal weight\nA_0 = {x: n*uniform_D[x] for x in D} # scale each probability by n to get synthetic data\n\nprint('Probability of age 20:', uniform_D[20])\nprint('Number of synthetic records of age 20:', A_0[20])\n```\n\nIn the code above, `uniform_D` represents a probability distribution, and `A_0` can be considered a *synthetic dataset*. If the probability that an individual's age is 20 is 1%, and the Adult dataset has 32,561 people, then the expected number of 20-year-olds in a synthetic dataset sampled from the distribution would be 325.61. We can run a range query on the synthetic data by adding up the values of `A_0` for ages in the range:\n\n```{python}\ndef range_query_synthetic(A, q):\n    lower, upper = q\n    return sum([A[age] for age in range(lower, upper)])\n\nrange_query_synthetic(A_0, Q[0])\n```\n\n# Step 1: Select a Query\n\nThe first step of the algorithm selects a query from the workload. The goal is to select the *worst* query: the one for which the current synthetic data gives an answer with the worst error. To do this, we use the [exponential mechanism](https://programming-dp.com/ch9.html), which allows selecting from a set of options in a differentially private way. Our implementation uses report noisy max, an easy-to-implement version of the exponential mechanism.\n\n![ss.png](attachment:ss.png)\n\nThe key challenge is defining the score function for the use of the exponential mechanism, and bounding its sensitivity. We define the score of a query $q \\in Q$ as:\n\n1. Calculate the query's result on the synthetic data ($q(A_{i-1})$ from Figure 1)\n2. Calculate the query's result on the real data ($q(B)$ from Figure 1)\n3. Return the absolute difference between them - the error of the synthetic data for this query\n\n```{python}\ndef mk_score_function(A_last):\n    def score_function(data, q):\n        syn_answer = range_query_synthetic(A_last, q)  # run the query on the synthetic data\n        real_answer = range_query(data, q)             # run the query on the real data\n        return abs(syn_answer - real_answer)          # score is the absolute difference\n    return score_function\n```\n\nWe use a nested function here because we will need to adjust the value of `A_last` as the algorithm runs. We can generate a score function for `A_0` and run it to see the error of the synthetic data on the first five queries of the workload:\n\n```{python}\nscore_fun = mk_score_function(A_0)\nfor q in Q[:5]:\n    print(q, score_fun(B, q))\n```\n\nUnder this scoring function, queries with large error will receive high scores. Picking the query with the maximum score corresponds to picking the query with the maximum error. The sensitivity of this function is 1, since `real_answer` is a range query with a sensitivity of 1. Thus we can use `report_noisy_max` to pick the highest-error query with differential privacy:\n\n```{python}\n#| colab: {base_uri: https://localhost:8080/}\nq_i = report_noisy_max(data = B, \n                       options = Q, \n                       score_function = mk_score_function(A_0), \n                       sensitivity = 1.0, \n                       epsilon = 1.0)\nq_i\n```\n\n# Step 2: Perform a Measurement\n\nThe second step of the algorithm performs a *measurement*: it calculates the result of the worst-error query on the real data, and adds Laplace noise to satisfy differential privacy. Again, the sensitivity is 1, because $q_i$ is a range query.\n\n![ss.png](attachment:ss.png)\n\n```{python}\nm_i = laplace_mech(range_query(B, q_i),\n                   sensitivity = 1,\n                   epsilon = 1.0)\nm_i\n```\n\n# Step 3: Multiplicative Weights\n\nThe final step of the algorithm calculates the new synthetic data $A_i$ based on the prior synthetic data $A_{i-1}$ using the [multiplicative weights update rule](https://en.wikipedia.org/wiki/Multiplicative_weight_update_method).\n\n![ss.png](attachment:ss.png)\n\nThe rule involves calculating a probability distribution for $A_i$ and then scaling it by $n$, as we did for `A_0`. The key idea behind the rule is to scale the probability of each element $x$ based on the (signed) difference between the measurement $m_i$ and the query's result on the old synthetic data. In Python, we can write:\n\n```{python}\ndef in_range(q, x): # indicator function: 1 if x is in range, 0 otherwise\n    lower, upper = q\n    return int(x >= lower and x < upper)\n\ndef mw_update(A_last, q_i, m_i):\n    A_i_p = {x: A_last[x] * np.exp(in_range(q_i, x) * (m_i - range_query_synthetic(A_last, q_i))/(2*n))\n             for x in D}                      # run the MW update rule\n    total = sum(A_i_p.values())               # calculate total for normalization\n    A_i = {x: n * A_i_p[x]/total for x in D}  # normalize, then scale by n\n    return A_i\n    \nA_1 = mw_update(A_0, q_i, m_i)\nprint('Number of 10-year-olds:', A_1[10])\nprint('Number of 20-year-olds:', A_1[20])\n```\n\nThe `mw_update` function implements the MW update rule, returning $A_i$. It's implemented in terms of `in_range`, an indicator function that implements $q_i(x)$ from Figure 1. The `A_i_p` dictionary defines the un-normalized PMF defined by the rule. After building the distribution, we normalize it (as directed by the $\\propto$ symbol in Figure 1) and scale it by $n$.\n\nAfter running the rule, notice that elements outside the range (e.g. 10-year-olds) have been scaled down, while elements inside the range (e.g. 20-year-olds) have been scaled up. By repeating this rule, the MWEM algorithm iteratively refines $A_i$ to more closely match the true data for the queries in $Q$.\n\n# Completing the Algorithm\n\nWe can now define the complete algorithm in terms of the steps we've written so far. The only change from the code above is the splitting of the privacy budget: we use $\\epsilon/(2*T)$ for each step, so that the total privacy budget equals $\\epsilon$ by sequential composition.\n\n```{python}\ndef mwem(B, Q, T, epsilon, n):\n    # Step 0: initialize A_i\n    A_0 = {x: n*uniform_D[x] for x in D} # scale each probability by n to get synthetic data\n    A_i = A_0\n    \n    for i in range(T):\n        if i % 10 == 0:\n            plt.plot(list(A_i.values()), label=f'Iteration {i}') # plot synthetic data\n            plt.legend()\n\n        # Step 1: Select a query\n        q_i = report_noisy_max(data = B, \n                               options = Q, \n                               score_function = mk_score_function(A_i), \n                               sensitivity = 1.0, \n                               epsilon = epsilon/(2*T))\n        \n        # Step 2: Perform a measurement\n        m_i = laplace_mech(range_query(B, q_i),\n                           sensitivity = 1,\n                           epsilon = epsilon/(2*T))\n        \n        # Step 3: Run MW update\n        A_i = mw_update(A_i, q_i, m_i)\n\n    return A_i\n```\n\nIn the code above, we also plot the synthetic data values in each iteration (with `plt.plot`), to visualize how the synthetic data changes over time.\n\n```{python}\nA_final = mwem(B, Q, 50, 1.0, len(adult))\n```\n\nThe resulting graph shows the progression from uniform data (iteration 0, blue line) to the final result after 25 iterations. The algorithm iteratively converges to a synthetic dataset that roughly mimics the distribution of ages in the Adult dataset. We can also see how the algorithm progresses from fitting the data to the coarse-grained queries (i.e large ranges) in early iterations, toward fine-tuning the data to fine-grained queries (i.e. small ranges) in later iterations. For example, iteration 10 (orange line) includes a coarse-grained estimate of the population between age 50 and age 70. Iteration 20 (green line) refines this part of the population.\n\n# When does MWEM Work?\n\nThe MWEM algorithm is simple and effective for *small domains*. In our example (ages in the Adult dataset), the domain size of 100 is no problem at all. For data with multiple dimensions, the domain representation needs to include *all combinations of attribute values*, so the domain size is exponential in the number of dimensions. For 2-dimensional data, MWEM can often work quite well; for higher-dimensional data, it can be difficult or impossible to represent the $A_i$ dictionary due to its size.\n\n","srcMarkdownNoYaml":"\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport random\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef laplace_mech(v, sensitivity, epsilon):\n    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n\ndef laplace_mech_vec(vec, sensitivity, epsilon):\n    return [v + np.random.laplace(loc=0, scale=sensitivity / epsilon) for v in vec]\n\ndef gaussian_mech(v, sensitivity, epsilon, delta):\n    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n\ndef gaussian_mech_vec(vec, sensitivity, epsilon, delta):\n    return [v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n            for v in vec]\n\ndef pct_error(orig, priv):\n    return np.abs(orig - priv)/orig * 100.0\n\nadult = pd.read_csv('https://github.com/jnear/cs3110-data-privacy/raw/main/homework/adult_with_pii.csv')\n\ndef report_noisy_max(data, options, score_function, sensitivity, epsilon):\n    # Calculate the score for each element of R\n    scores = [score_function(data, r) for r in options]\n\n    # Add noise to each score\n    noisy_scores = [laplace_mech(score, sensitivity, epsilon) for score in scores]\n\n    # Find the index of the maximum score\n    max_idx = np.argmax(noisy_scores)\n\n    # Return the element corresponding to that index\n    return options[max_idx]\n```\n\n\nThe Multiplicative Weights with Exponential Mechanism (MWEM) mechanism is a differentially private, workload-aware algorithm for modeling the distribution of a private dataset. It can be used to answer query workloads, to generate synthetic data, or to build models that could be used for other purposes. The algorithm was original presented in the paper [A Simple and Practical Algorithm for Differentially Private Data Release](https://proceedings.neurips.cc/paper_files/paper/2012/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf) by Moritz Hardt, Katrina Ligett, and Frank McSherry. MWEM has remained an influential approach since its introduction, since it's simple and easy to implement but also works well in many cases.\n\nThe complete algorithm (screenshotted from the paper) appears below, in Figure 1. The inputs to the algorithm are:\n\n- The (private) dataset $B$\n- A set $Q$ of linear queries, often called the query workload\n- The number of iterations $T$ to run the algorithm\n- The privacy budget $\\epsilon$\n- The number of data records $n$\n\nThe algorithm outputs a *probability distribution* over all possible data points in $D$. Sampling from this distribution yields synthetic data; it can also be used directly to answer arbitrary linear queries. The rest of this post describes and implements the pieces of the algorithm.\n\n\n![ss.png](attachment:ss.png)\n\n# The Private Data\n\nThe algorithm's first argument, $B$, is a private dataset. For our implementation, we'll use the `Age` column of the Adult dataset.\n\n```{python}\nB = adult['Age']\nB.hist();\n```\n\n# The Query Workload\n\nThe algorithm's second argument, $Q$, is a set of linear queries. Here, a *linear* query refers to a query that can be written as a weighted sum of elements; counts and sums are typical examples of linear queries. $Q$ in the algorithm is a set of these queries, and is often called a *query workload* in other papers. \n\nIn a workload-aware algorithm like MWEM, the goal is to build a model that does a good job answering the queries in the workload, under the assumption that the analyst has used domain-specific knowledge to carefully design the workload so it includes all of the most important queries. In practice, generating a good workload is actually really difficult, because most analysts don't know exactly what the data will be used for when building the differentially private data release. Most papers use simple stand-ins for the workload, like 2- or 3-way marginal queries, prefix queries, or range queries.\n\nOur implementation supports arbitrary linear queries, but we'll use random [range queries](https://programming-dp.com/ch10.html#application-range-queries) as our workload. Specifically, our range queries will count the number of individuals in the dataset whose ages lie within an interval. The `range_query` function implements a range query on a pandas dataframe.\n\n```{python}\n#| colab: {base_uri: https://localhost:8080/}\ndef range_query(data, q):\n    lower, upper = q\n    return len(data[(data >= lower) & (data < upper)])\n```\n\nWe generate 100 random range queries for the `Age` column of the Adult dataset. We represent the queries using the upper and lower bound for each range. We can run these queries by calling the `range_query` function: the code below generates the queries, runs `range_query` to get the answers, and prints the first 5 elements of each.\n\n```{python}\nNUM_QUERIES = 100\nrandom_lower_bounds = [random.randint(1, 70) for _ in range(NUM_QUERIES)]\nQ = [(lb, random.randint(lb, 100)) for lb in random_lower_bounds]\nreal_answers = [range_query(B, q) for q in Q]\nprint('First 5 queries: ', Q[:5])\nprint('First 5 answers: ', real_answers[:5])\n```\n\n# Step 0: Initialize the Model\n\nThe first step of the algorithm says: \"Let $A_0$ denote $n$ times the uniform distribution over $D$.\" What does that mean?\n\nIn our case, where the dataset is the `Age` column of the Adult dataset, $D$ is the universe of all possible ages. If we assume nobody is older than 100 years old, then we could use the numbers from 0 to 100 for our universe.\n\n```{python}\nD = list(range(0, 100))\n```\n\nSince $D$ is finite, we can represent a probability distribution over $D$ as a mapping from each element of $D$ to its probability (i.e. a [probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function) (PMF)). The PMF for the uniform distribution in this case assigns equal probability to each element of $D$. Since we have 100 ages, the probability for each one is 1%. Finally, $n$ times this distribution scales each entry by $n$.\n\n```{python}\nn = len(adult)\nuniform_D = {x: 1/len(D) for x in D} # give each record equal weight\nA_0 = {x: n*uniform_D[x] for x in D} # scale each probability by n to get synthetic data\n\nprint('Probability of age 20:', uniform_D[20])\nprint('Number of synthetic records of age 20:', A_0[20])\n```\n\nIn the code above, `uniform_D` represents a probability distribution, and `A_0` can be considered a *synthetic dataset*. If the probability that an individual's age is 20 is 1%, and the Adult dataset has 32,561 people, then the expected number of 20-year-olds in a synthetic dataset sampled from the distribution would be 325.61. We can run a range query on the synthetic data by adding up the values of `A_0` for ages in the range:\n\n```{python}\ndef range_query_synthetic(A, q):\n    lower, upper = q\n    return sum([A[age] for age in range(lower, upper)])\n\nrange_query_synthetic(A_0, Q[0])\n```\n\n# Step 1: Select a Query\n\nThe first step of the algorithm selects a query from the workload. The goal is to select the *worst* query: the one for which the current synthetic data gives an answer with the worst error. To do this, we use the [exponential mechanism](https://programming-dp.com/ch9.html), which allows selecting from a set of options in a differentially private way. Our implementation uses report noisy max, an easy-to-implement version of the exponential mechanism.\n\n![ss.png](attachment:ss.png)\n\nThe key challenge is defining the score function for the use of the exponential mechanism, and bounding its sensitivity. We define the score of a query $q \\in Q$ as:\n\n1. Calculate the query's result on the synthetic data ($q(A_{i-1})$ from Figure 1)\n2. Calculate the query's result on the real data ($q(B)$ from Figure 1)\n3. Return the absolute difference between them - the error of the synthetic data for this query\n\n```{python}\ndef mk_score_function(A_last):\n    def score_function(data, q):\n        syn_answer = range_query_synthetic(A_last, q)  # run the query on the synthetic data\n        real_answer = range_query(data, q)             # run the query on the real data\n        return abs(syn_answer - real_answer)          # score is the absolute difference\n    return score_function\n```\n\nWe use a nested function here because we will need to adjust the value of `A_last` as the algorithm runs. We can generate a score function for `A_0` and run it to see the error of the synthetic data on the first five queries of the workload:\n\n```{python}\nscore_fun = mk_score_function(A_0)\nfor q in Q[:5]:\n    print(q, score_fun(B, q))\n```\n\nUnder this scoring function, queries with large error will receive high scores. Picking the query with the maximum score corresponds to picking the query with the maximum error. The sensitivity of this function is 1, since `real_answer` is a range query with a sensitivity of 1. Thus we can use `report_noisy_max` to pick the highest-error query with differential privacy:\n\n```{python}\n#| colab: {base_uri: https://localhost:8080/}\nq_i = report_noisy_max(data = B, \n                       options = Q, \n                       score_function = mk_score_function(A_0), \n                       sensitivity = 1.0, \n                       epsilon = 1.0)\nq_i\n```\n\n# Step 2: Perform a Measurement\n\nThe second step of the algorithm performs a *measurement*: it calculates the result of the worst-error query on the real data, and adds Laplace noise to satisfy differential privacy. Again, the sensitivity is 1, because $q_i$ is a range query.\n\n![ss.png](attachment:ss.png)\n\n```{python}\nm_i = laplace_mech(range_query(B, q_i),\n                   sensitivity = 1,\n                   epsilon = 1.0)\nm_i\n```\n\n# Step 3: Multiplicative Weights\n\nThe final step of the algorithm calculates the new synthetic data $A_i$ based on the prior synthetic data $A_{i-1}$ using the [multiplicative weights update rule](https://en.wikipedia.org/wiki/Multiplicative_weight_update_method).\n\n![ss.png](attachment:ss.png)\n\nThe rule involves calculating a probability distribution for $A_i$ and then scaling it by $n$, as we did for `A_0`. The key idea behind the rule is to scale the probability of each element $x$ based on the (signed) difference between the measurement $m_i$ and the query's result on the old synthetic data. In Python, we can write:\n\n```{python}\ndef in_range(q, x): # indicator function: 1 if x is in range, 0 otherwise\n    lower, upper = q\n    return int(x >= lower and x < upper)\n\ndef mw_update(A_last, q_i, m_i):\n    A_i_p = {x: A_last[x] * np.exp(in_range(q_i, x) * (m_i - range_query_synthetic(A_last, q_i))/(2*n))\n             for x in D}                      # run the MW update rule\n    total = sum(A_i_p.values())               # calculate total for normalization\n    A_i = {x: n * A_i_p[x]/total for x in D}  # normalize, then scale by n\n    return A_i\n    \nA_1 = mw_update(A_0, q_i, m_i)\nprint('Number of 10-year-olds:', A_1[10])\nprint('Number of 20-year-olds:', A_1[20])\n```\n\nThe `mw_update` function implements the MW update rule, returning $A_i$. It's implemented in terms of `in_range`, an indicator function that implements $q_i(x)$ from Figure 1. The `A_i_p` dictionary defines the un-normalized PMF defined by the rule. After building the distribution, we normalize it (as directed by the $\\propto$ symbol in Figure 1) and scale it by $n$.\n\nAfter running the rule, notice that elements outside the range (e.g. 10-year-olds) have been scaled down, while elements inside the range (e.g. 20-year-olds) have been scaled up. By repeating this rule, the MWEM algorithm iteratively refines $A_i$ to more closely match the true data for the queries in $Q$.\n\n# Completing the Algorithm\n\nWe can now define the complete algorithm in terms of the steps we've written so far. The only change from the code above is the splitting of the privacy budget: we use $\\epsilon/(2*T)$ for each step, so that the total privacy budget equals $\\epsilon$ by sequential composition.\n\n```{python}\ndef mwem(B, Q, T, epsilon, n):\n    # Step 0: initialize A_i\n    A_0 = {x: n*uniform_D[x] for x in D} # scale each probability by n to get synthetic data\n    A_i = A_0\n    \n    for i in range(T):\n        if i % 10 == 0:\n            plt.plot(list(A_i.values()), label=f'Iteration {i}') # plot synthetic data\n            plt.legend()\n\n        # Step 1: Select a query\n        q_i = report_noisy_max(data = B, \n                               options = Q, \n                               score_function = mk_score_function(A_i), \n                               sensitivity = 1.0, \n                               epsilon = epsilon/(2*T))\n        \n        # Step 2: Perform a measurement\n        m_i = laplace_mech(range_query(B, q_i),\n                           sensitivity = 1,\n                           epsilon = epsilon/(2*T))\n        \n        # Step 3: Run MW update\n        A_i = mw_update(A_i, q_i, m_i)\n\n    return A_i\n```\n\nIn the code above, we also plot the synthetic data values in each iteration (with `plt.plot`), to visualize how the synthetic data changes over time.\n\n```{python}\nA_final = mwem(B, Q, 50, 1.0, len(adult))\n```\n\nThe resulting graph shows the progression from uniform data (iteration 0, blue line) to the final result after 25 iterations. The algorithm iteratively converges to a synthetic dataset that roughly mimics the distribution of ages in the Adult dataset. We can also see how the algorithm progresses from fitting the data to the coarse-grained queries (i.e large ranges) in early iterations, toward fine-tuning the data to fine-grained queries (i.e. small ranges) in later iterations. For example, iteration 10 (orange line) includes a coarse-grained estimate of the population between age 50 and age 70. Iteration 20 (green line) refines this part of the population.\n\n# When does MWEM Work?\n\nThe MWEM algorithm is simple and effective for *small domains*. In our example (ages in the Adult dataset), the domain size of 100 is no problem at all. For data with multiple dimensions, the domain representation needs to include *all combinations of attribute values*, so the domain size is exponential in the number of dimensions. For 2-dimensional data, MWEM can often work quite well; for higher-dimensional data, it can be difficult or impossible to represent the $A_i$ dictionary due to its size.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.24","theme":["cosmo","brand"],"title-block-banner":true,"title":"MWEM","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}